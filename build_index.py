import os
import json
import gzip
import boto3
from dotenv import load_dotenv
from tqdm import tqdm

from langchain_aws import BedrockEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader

# --- Configuraci√≥n ---
load_dotenv()
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME")
bedrock_client = boto3.client(service_name='bedrock-runtime', region_name='eu-central-1')

print("üöÄ RAG Documentation Navigator - Optimized Index Builder")
print(f"üìç Regi√≥n: eu-central-1 | ü™£ Bucket: {S3_BUCKET_NAME}")
print("="*60)

def create_optimized_index():
    # Paso 1: Cargar documentos
    print("\nüìö [1/6] Cargando documentos PDF...")
    loader = DirectoryLoader('./data/', glob="**/*.pdf", loader_cls=PyPDFLoader, show_progress=True)
    documents = loader.load()
    
    if not documents:
        print("‚ùå ERROR: No se encontraron documentos en la carpeta 'data/'")
        print("   Aseg√∫rate de tener los PDFs en ./data/")
        return
    
    print(f"‚úÖ {len(documents)} documentos cargados exitosamente")
    
    # Mostrar qu√© documentos se cargaron
    unique_sources = set()
    for doc in documents:
        if 'source' in doc.metadata:
            unique_sources.add(doc.metadata['source'])
    
    print("\nüìÑ Documentos procesados:")
    for source in unique_sources:
        filename = os.path.basename(source)
        print(f"   ‚Ä¢ {filename}")
    
    # Paso 2: Dividir en chunks
    print("\n‚úÇÔ∏è [2/6] Dividiendo documentos en chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " ", ""],
        length_function=len
    )
    docs = text_splitter.split_documents(documents)
    print(f"‚úÖ Documentos divididos en {len(docs)} chunks")
    
    # Paso 3: Inicializar modelo de embeddings
    print("\nü§ñ [3/6] Inicializando Amazon Titan Embeddings...")
    embeddings_model = BedrockEmbeddings(
        client=bedrock_client,
        model_id="amazon.titan-embed-text-v1"
    )
    print("‚úÖ Modelo de embeddings listo")
    
    # Paso 4: Generar embeddings
    print("\nüßÆ [4/6] Generando embeddings vectoriales...")
    print("   (Esto puede tomar varios minutos)")
    
    index_data = {
        "chunks": [],
        "metadata": {
            "total_chunks": len(docs),
            "embedding_model": "amazon.titan-embed-text-v1",
            "embedding_dimension": 1536,
            "chunk_size": 1000,
            "chunk_overlap": 100,
            "created_date": str(os.popen('date').read().strip())
        }
    }
    
    # Procesar en batches para mostrar progreso
    batch_size = 10
    for i in tqdm(range(0, len(docs), batch_size), desc="Procesando"):
        batch = docs[i:i+batch_size]
        
        for j, doc in enumerate(batch):
            doc_index = i + j
            
            # Generar embedding
            try:
                embedding = embeddings_model.embed_query(doc.page_content)
                
                # Limpiar metadata para hacerla m√°s compacta
                clean_metadata = {
                    "source": os.path.basename(doc.metadata.get('source', 'unknown')),
                    "page": doc.metadata.get('page', 0)
                }
                
                # Agregar al √≠ndice
                index_data["chunks"].append({
                    "id": doc_index,
                    "text": doc.page_content[:1000],  # Limitar texto para reducir tama√±o
                    "embedding": embedding,
                    "metadata": clean_metadata
                })
                
            except Exception as e:
                print(f"\n‚ö†Ô∏è Error en chunk {doc_index}: {str(e)}")
                continue
    
    successful_chunks = len(index_data["chunks"])
    print(f"\n‚úÖ Embeddings generados: {successful_chunks}/{len(docs)} chunks")
    
    # Paso 5: Comprimir y guardar localmente
    print("\nüíæ [5/6] Comprimiendo y guardando √≠ndice...")
    
    # Crear directorio si no existe
    os.makedirs("local_index", exist_ok=True)
    
    # Convertir a JSON y comprimir
    json_data = json.dumps(index_data)
    json_size_mb = len(json_data.encode('utf-8')) / (1024 * 1024)
    print(f"   Tama√±o JSON sin comprimir: {json_size_mb:.2f} MB")
    
    compressed_data = gzip.compress(json_data.encode('utf-8'), compresslevel=9)
    compressed_size_mb = len(compressed_data) / (1024 * 1024)
    print(f"   Tama√±o comprimido: {compressed_size_mb:.2f} MB")
    print(f"   Ratio de compresi√≥n: {(1 - compressed_size_mb/json_size_mb)*100:.1f}%")
    
    # Guardar localmente
    local_path = "local_index/index.json.gz"
    with open(local_path, "wb") as f:
        f.write(compressed_data)
    print(f"‚úÖ √çndice guardado localmente en: {local_path}")
    
    # Paso 6: Subir a S3
    print(f"\n‚òÅÔ∏è [6/6] Subiendo √≠ndice a S3...")
    s3_client = boto3.client('s3', region_name='eu-central-1')
    
    try:
        # Subir con metadata √∫til
        s3_client.put_object(
            Bucket=S3_BUCKET_NAME,
            Key="index.json.gz",
            Body=compressed_data,
            ContentType="application/gzip",
            Metadata={
                'chunks': str(successful_chunks),
                'original-size-mb': f"{json_size_mb:.2f}",
                'compressed-size-mb': f"{compressed_size_mb:.2f}"
            }
        )
        print(f"‚úÖ √çndice subido exitosamente a: s3://{S3_BUCKET_NAME}/index.json.gz")
        
        # Verificar que se subi√≥ correctamente
        response = s3_client.head_object(Bucket=S3_BUCKET_NAME, Key="index.json.gz")
        s3_size_mb = response['ContentLength'] / (1024 * 1024)
        print(f"‚úÖ Verificaci√≥n: Archivo en S3 tiene {s3_size_mb:.2f} MB")
        
    except Exception as e:
        print(f"‚ùå Error subiendo a S3: {e}")
        print("   Verifica que el bucket existe y tienes permisos")
        return
    
    # Resumen final
    print("\n" + "="*60)
    print("üéâ ¬°√çNDICE OPTIMIZADO CREADO CON √âXITO!")
    print("="*60)
    print(f"""
üìä Resumen:
   ‚Ä¢ Documentos procesados: {len(unique_sources)}
   ‚Ä¢ Total de chunks: {successful_chunks}
   ‚Ä¢ Tama√±o final: {compressed_size_mb:.2f} MB (vs ~99 MB con FAISS)
   ‚Ä¢ Reducci√≥n: {(1 - compressed_size_mb/99)*100:.1f}% m√°s peque√±o
   ‚Ä¢ Ubicaci√≥n S3: s3://{S3_BUCKET_NAME}/index.json.gz
   
üöÄ Siguiente paso: Actualizar la funci√≥n Lambda con el c√≥digo optimizado
""")

if __name__ == "__main__":
    create_optimized_index()